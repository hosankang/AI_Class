# -*- coding: utf-8 -*-
"""char_rnn.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Ij7zSzt9RfeGvechNbaNGIXLQHRWz0H_
"""

from __future__ import print_function 
from keras.callbacks import LambdaCallback
from keras.models import Sequential
from keras.layers import Dense, Activation
from keras.layers import LSTM
from keras.optimizers import RMSprop
import numpy as np
import random
import sys
import io

from google.colab import drive
drive.mount('/gdrive')

!ls /gdrive/My\ Drive/Colab\ Notebooks

text = open('/gdrive/My Drive/Colab Notebooks/linux_kernel_code.txt', 'r').read().lower() #리눅스 c코드 
print('text length', len(text)) #코드 총 길이

print(text[:1000]) #1000개만 확인

chars = sorted(list(set(text))) #텍스트에 있는 고유 char들을 가져옴 
print(chars)
print('total chars: ', len(chars)) #고유 char의 갯수

#2개의 사전 생성 
char_indices = dict((c,i)for i,c in enumerate(chars)) #char형 사전
indices_char = dict((i,c)for i,c in enumerate(chars)) #int형 사전
print(char_indices)
print('\n',indices_char)

#시퀀스
maxlen = 40 #시퀀스 40자 
step = 3
sentences = []
next_chars = []
for i in range(0, len(text) - maxlen, step):
    sentences.append(text[i: i+ maxlen])
    next_chars.append(text[i + maxlen])
print('sequences: ', len(sentences))

print(sentences[:3])
print(next_chars[:3])

x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)
y = np.zeros((len(sentences), len(chars)), dtype=np.bool)
for i, sentence in enumerate(sentences):
    for t, char in enumerate(sentence):
        x[i, t, char_indices[char]] = 1
    y[i, char_indices[next_chars[i]]] = 1

#모델
model = Sequential()
model.add(LSTM(128, input_shape=(maxlen, len(chars))))
model.add(Dense(len(chars)))
model.add(Activation('softmax'))
model.summary()

#모델 컴파일
model.compile(loss='categorical_crossentropy', optimizer='adam')

def sample(preds, temperature=1.0):
    preds = np.asarray(preds).astype('float64')
    preds = np.log(preds) / temperature
    exp_preds = np.exp(preds)
    preds = exp_preds / np.sum(exp_preds)
    probas = np.random.multinomial(1, preds, 1)
    return np.argmax(probas)

#학습마다 학습된 예측 코드 생성 
def on_epoch_end(epoch, logs):
    print()
    print('Epoch: %d '%epoch)
    start_index = random.randint(0, len(text) - maxlen -1)
    for diversity in [0.2, 0.5, 1.0, 1.2]:
        print('diversity: ',diversity)
        
        generated = ''
        sentence = text[start_index: start_index + maxlen]
        generated += sentence
        print('Generating with seed: "' + sentence + '"')
        sys.stdout.write(generated)
        
        for i in range(300):
            x_pred = np.zeros((1, maxlen, len(chars)))
            for t,char in enumerate(sentence):
                x_pred[0, t, char_indices[char]]= 1
                
            preds = model.predict(x_pred, verbose=0)[0]
            next_index = sample(preds, diversity)
            next_char = indices_char[next_index]
            
            generated += next_char
            sentence = sentence[1:] + next_char
            
            sys.stdout.write(next_char)
            sys.stdout.flush()
        print()
print_callback = LambdaCallback(on_epoch_end=on_epoch_end)

#체크포인트 콜백
from keras.callbacks import ModelCheckpoint

filepath = "weights.hdf5"
checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose =1,save_best_only=True, mode='min')

#Epoch마다 학습된 예상코드 불러오는 콜백
from keras.callbacks import ReduceLROnPlateau
reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.2, patience = 5, min_lr = 0.001)

callbacks = [print_callback, checkpoint, reduce_lr]

#학습 <배치 128, 에폭 40>
model.fit(x,y,batch_size= 128, epochs = 40, callbacks = callbacks)

#코드 생성 
def generate_text(length, diversity):
    start_index = random.randint(0, len(text) - maxlen -1)
    generated = ''
    sentence = text[start_index: start_index + maxlen]
    generated += sentence
    for i in range(length):
        x_pred = np.zeros((1, maxlen, len(chars)))
        for t,char in enumerate(sentence):
            x_pred[0, t, char_indices[char]]= 1.
                
        preds = model.predict(x_pred, verbose=0)[0]
        next_index = sample(preds, diversity)
        next_char = indices_char[next_index]
            
        generated += next_char
        sentence = sentence[1:] + next_char
    return generated

print('0.2')
print(generate_text(1000,0.2))
print('\n\n0.5:')
print(generate_text(1000,0.5))
print('\n\n1.0')
print(generate_text(1000,1.0))
print('\n\n1.2')
print(generate_text(1000,1.2))

